{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_FILENAME = \"validation_model_completions_step_{step}.csv\"\n",
    "TEST_FILENAME = \"test_model_completions_step_{step}.csv\"\n",
    "\n",
    "def get_accuracy_from_df(df):\n",
    "    return df['is_correct'].sum() / len(df)\n",
    "\n",
    "\n",
    "def get_test_acc(results_dir, eval_steps, num_samples=None):\n",
    "    try:\n",
    "        last_step = eval_steps[-1]\n",
    "        test_step_last_df = pd.read_csv(f\"{results_dir}/{TEST_FILENAME.format(step=last_step)}\")\n",
    "\n",
    "        if num_samples is not None:\n",
    "            # group by 'prompt_id' and take the first num_samples rows for each prompt_id\n",
    "            test_step_last_df = test_step_last_df.groupby('prompt').head(num_samples)\n",
    "\n",
    "        # print(len(test_step_520_df))\n",
    "\n",
    "        return get_accuracy_from_df(test_step_last_df)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_all_results_from_format(\n",
    "        exp_runs, base_exp_dir, eval_group_name, datasets, val_filename, test_filename, \n",
    "        bootstrap_iters=[1, 2, 3, 4], trials=[0, 1, 2], seeds=[42, 43, 44, 45, 46], eval_steps=[520], num_samples=None\n",
    "    ):\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over methods and datasets\n",
    "    for method, base_dir_format in exp_runs.items():\n",
    "        for dataset in datasets:\n",
    "            format_key = (method, dataset)\n",
    "\n",
    "            if format_key not in results:\n",
    "                results[format_key] = []\n",
    "            \n",
    "            # Generate all combinations of variables\n",
    "            if method == \"latent_bootstrap\":\n",
    "                for iter_val, trial_val, seed_val in itertools.product(bootstrap_iters, trials, seeds):\n",
    "                    try:\n",
    "                        results_dir = os.path.join(base_exp_dir, base_dir_format.format(iter=iter_val, trial=trial_val, eval_name=eval_group_name.format(dataset=dataset), seed=seed_val))\n",
    "                        test_acc = get_test_acc(results_dir, num_samples=num_samples, eval_steps=eval_steps)\n",
    "\n",
    "                        if test_acc is None:\n",
    "                            print(f\"File not found for method {method}, dataset {dataset}, iter {iter_val}, trial {trial_val}, seed {seed_val}: {results_dir}\")\n",
    "                            continue\n",
    "\n",
    "                        results[format_key].append((iter_val, trial_val, seed_val, test_acc))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {iter_val}/{trial_val}/{seed_val}: {e}\")\n",
    "            else:\n",
    "                for trial_val, seed_val in itertools.product(trials, seeds):\n",
    "                    try:\n",
    "                        results_dir = os.path.join(base_exp_dir, base_dir_format.format(trial=trial_val, eval_name=eval_group_name.format(dataset=dataset), seed=seed_val))\n",
    "                        test_acc = get_test_acc(results_dir, num_samples=num_samples, eval_steps=eval_steps)\n",
    "\n",
    "                        if test_acc is None:\n",
    "                            print(f\"File not found for method {method}, dataset {dataset}, trial {trial_val}, seed {seed_val}: {results_dir}\")\n",
    "                            continue\n",
    "\n",
    "                        results[format_key].append((trial_val, seed_val, test_acc))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {trial_val}/{seed_val}: {e}\")\n",
    "                \n",
    "    df_results = {}\n",
    "    for key, value in results.items():\n",
    "        if key[0] == 'latent_bootstrap':\n",
    "            df_results[key] = pd.DataFrame(value, columns=['iter', 'trial', 'seed', 'accuracy'])\n",
    "        else:\n",
    "            df_results[key] = pd.DataFrame(value, columns=['trial', 'seed', 'accuracy'])\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_MAP = {\n",
    "    \"latent_bootstrap\": 'Latent Bootstrap', \n",
    "    \"raw_flops_matched_baseline\": 'Raw Train-FLOP-Match', \n",
    "    \"warmstart\": 'Latent Warmstart', \n",
    "    \"raw_token_matched_baseline\": 'Raw Token-Match',\n",
    "}\n",
    "\n",
    "DATASET_MAP = {\n",
    "    \"math\": 'MATH (Fine-tuned)',\n",
    "    \"gsm8k\": 'GSM8K (Fine-tuned)',\n",
    "}\n",
    "\n",
    "RUN_ORDER = list(RUN_MAP.keys())\n",
    "COLOR_PALETTE = sns.color_palette()\n",
    "\n",
    "# Plot the results from model selection\n",
    "def plot_fine_tuning_results(selected_results, datasets, unify_legend=True):\n",
    "    n_cols, n_rows = len(datasets), 1\n",
    "    unit_figsize = (6, 4)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(unit_figsize[0]*n_cols, unit_figsize[1]*n_rows))\n",
    "    if n_cols * n_rows == 1:\n",
    "        axes = [axes]\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        # Create plot for each dataset\n",
    "        ax = axes[idx]\n",
    "        # Check if we have bootstrap data for this dataset\n",
    "        bootstrap_df = selected_results[('latent_bootstrap', dataset)]\n",
    "        assert 'iter' in bootstrap_df.columns, f\"No iter column in bootstrap data for {dataset}\"\n",
    "\n",
    "        sns.lineplot(x='iter', y='accuracy', data=bootstrap_df, ax=ax, errorbar=\"se\", marker=\"o\", markersize=10, linewidth=3, label=RUN_MAP['latent_bootstrap'], color=COLOR_PALETTE[RUN_ORDER.index('latent_bootstrap')])\n",
    "        \n",
    "        # Plot other experiment types as horizontal lines\n",
    "        x_min, x_max = ax.get_xlim()\n",
    "        \n",
    "        for i, exp_type in enumerate(['warmstart', 'raw_flops_matched_baseline', 'raw_token_matched_baseline']):\n",
    "            df = selected_results[(exp_type, dataset)]\n",
    "            if len(df) == 0:\n",
    "                print(f\"No {exp_type} data for {dataset}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate mean, std, and standard error of test accuracy\n",
    "            mean_test_acc = df['accuracy'].mean()\n",
    "            std_test_acc = df['accuracy'].std()\n",
    "            se_test_acc = std_test_acc / np.sqrt(len(df))  # Calculate standard error\n",
    "            \n",
    "            run_color = COLOR_PALETTE[RUN_ORDER.index(exp_type)]\n",
    "            ax.axhline(y=mean_test_acc, color=run_color, linestyle='--', label=RUN_MAP[exp_type], linewidth=3)\n",
    "            \n",
    "            # Add shaded region for standard error\n",
    "            ax.fill_between(\n",
    "                [x_min - 0.5, x_max + 0.5], \n",
    "                mean_test_acc - se_test_acc,  # Use standard error instead of std\n",
    "                mean_test_acc + se_test_acc,  # Use standard error instead of std\n",
    "                color=run_color, alpha=0.2)\n",
    "            \n",
    "        \n",
    "        x_min, x_max = ax.get_xlim()\n",
    "        ax.set_xlim(np.ceil(x_min) - 0.1, np.floor(x_max) + 0.1)\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Bootstrap Iteration')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title(DATASET_MAP[dataset])\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        all_bootstrap_iters = bootstrap_df['iter'].unique()\n",
    "        ax.set_xticks(all_bootstrap_iters)\n",
    "        ax.set_xticklabels(all_bootstrap_iters)\n",
    "\n",
    "        if not unify_legend:\n",
    "            ax.legend()\n",
    "            legend = ax.get_legend()\n",
    "        else:\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "\n",
    "    if unify_legend:\n",
    "        # Adjust the figure size to accommodate the legend at the top\n",
    "        plt.gcf().set_size_inches(unit_figsize[0]*n_cols, unit_figsize[1]*n_rows + 1)  # Added extra height instead of width\n",
    "        # Create unified legend at the top\n",
    "        handles, labels = axes[0].get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, bbox_to_anchor=(0.5, 1.0), loc='lower center', ncol=2, fontsize=SIZE_LARGE)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_RUN_TO_BASE_DUMP_DIR_FORMAT = {\n",
    "    \"latent_bootstrap\": \"train_bootstrap_fixed_data_bootstrap/train_bootstrap_fixed_data_bootstrap_setup=bootstrap_latents_iter={iter}_mc=4_scratch_trial_{trial}/{eval_name}/0000033060/seed={seed}\",\n",
    "    \"raw_token_matched_baseline\": \"train_bootstrap_fixed_data_bootstrap/train_bootstrap_fixed_data_bootstrap_setup=raw_token_matched_scratch_trial_{trial}/{eval_name}/0000009765/seed={seed}\",\n",
    "    \"raw_flops_matched_baseline\": \"train_bootstrap_fixed_data_bootstrap/train_bootstrap_fixed_data_bootstrap_setup=raw_flops_matched_scratch_trial_{trial}/{eval_name}/0000033060/seed={seed}\",\n",
    "    # \"warmstart\": \"train_bootstrap_fixed_data_warmstart/train_bootstrap_fixed_data_warmstart_latent=random_opt=cosine_lr=1e-4_240m_raw_trial_{trial}/{eval_name}/0000004096/seed={seed}\",\n",
    "}\n",
    "\n",
    "BASE_EXP_DIR = \"../exp_logs\"\n",
    "EVAL_GROUP_NAME = \"finetune_eval_on_{dataset}\"\n",
    "EVAL_DATASETS = [\"math\"]\n",
    "\n",
    "LOAD_RESULTS_KWARGS = {\n",
    "    \"exp_runs\": EXP_RUN_TO_BASE_DUMP_DIR_FORMAT,\n",
    "    \"base_exp_dir\": BASE_EXP_DIR,\n",
    "    \"eval_group_name\": EVAL_GROUP_NAME,\n",
    "    \"datasets\": EVAL_DATASETS,\n",
    "    \"val_filename\": VAL_FILENAME,\n",
    "    \"test_filename\": TEST_FILENAME,\n",
    "    \"bootstrap_iters\": [1, 2, 3, 4],\n",
    "    \"trials\": [0, 1, 2],\n",
    "    \"seeds\": [42, 43, 44, 45, 46],\n",
    "    \"eval_steps\": [520],\n",
    "    \"num_samples\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_results = load_all_results_from_format(**LOAD_RESULTS_KWARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_fine_tuning_results(formatted_results, datasets=EVAL_DATASETS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lingua_math_250116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
