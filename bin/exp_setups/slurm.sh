# Modify the following sphinx setups to run your experiments!

## Training: typically a node with 8 H200 GPUs
TRAIN_ACCOUNT=<YOUR_ACCOUNT_FOR_TRAINING>
TRAIN_PARTITION=<YOUR_PARTITION_FOR_TRAINING>
TRAIN_EXTRA_FLAGS=""


## Eval: can be distributed across multiple nodes with standard gpus like A100 and A6000
EVAL_ACCOUNT=<YOUR_ACCOUNT_FOR_EVAL>
EVAL_PARTITION=<YOUR_PARTITION_FOR_EVAL>
EVAL_EXCLUDE=<YOUR_EXCLUDE_NODES_FOR_EVAL>
EVAL_EXTRA_FLAGS=""
EVAL_CONFIG_ARGS="slurm.account=${EVAL_ACCOUNT} slurm.partition=${EVAL_PARTITION} slurm.exclude=${EVAL_EXCLUDE} ${EVAL_EXTRA_FLAGS}"


## Generation: can be distributed over multiple nodes with idle gpus like A100, A6000, A5000, RTX3090, etc.
GEN_ACCOUNT=<YOUR_ACCOUNT_FOR_GEN>
GEN_PARTITION=<YOUR_PARTITION_FOR_GEN>
GEN_EXTRA_FLAGS=<YOUR_EXTRA_FLAGS_FOR_GEN>


## Finetuning: typically 1 A100/H100 GPU
FINETUNE_ACCOUNT=<YOUR_ACCOUNT_FOR_FINETUNING>
FINETUNE_PARTITION=<YOUR_PARTITION_FOR_FINETUNING>
FINETUNE_EXTRAT_ENV_VARS=<YOUR_EXTRA_ENV_VARS_FOR_FINETUNING>  # such as cache dirs
FINETUNE_EXTRA_FLAGS=<YOUR_EXTRA_FLAGS_FOR_FINETUNING>